{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPimewwBWvAf+Oq8YeqF84K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeevfromkrec/NLP/blob/master/text%20processing%20and%20prediction%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suA8sw0fu-Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "755ec432-a77d-423d-e925-7abdb8cc97b3"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-25 16:09:14--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  1.22MB/s    in 2.5s    \n",
            "\n",
            "2020-07-25 16:09:17 (1.22 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUvNlXJFvCMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzf review_polarity.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjY_xWHmvIWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcaba5e8-1203-4da4-d07d-4d276fcbc3cd"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "poldata.README.2.0  review_polarity.tar.gz  sample_data  txt_sentoken\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7nx-bNqvJtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4cf58b66-7945-486a-d44a-20365cefe7f2"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAFZXtCxv4u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename): \n",
        "# open the file as read only \n",
        "  file = open(filename, 'r') \n",
        "# read all text \n",
        "  text = file.read() \n",
        "# close the file \n",
        "  file.close()\n",
        "  return text\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uvP7L--wIbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "  tokens=doc.split()\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [word for word in tokens if len(word) > 1] \n",
        "  return tokens         "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FfHXDVmw1Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_to_lines(filename, vocab):\n",
        "  doc=load_doc(filename)\n",
        "  tokens=clean_doc(doc)\n",
        "  tokens=[ w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a2H9WX_7Ox4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_list(lines, filename): \n",
        "# convert lines to a single blob of text \n",
        "  data = '\\n'.join(lines) \n",
        "  # open file \n",
        "  file = open(filename, 'w') \n",
        "  # write text \n",
        "  file.write(data) \n",
        "  # close file \n",
        "  file.close()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WCIxnH040ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "46eee175-fefb-4ee0-8ae4-d3d166c7e316"
      },
      "source": [
        "# define vocab \n",
        "vocab = Counter() \n",
        "# add all docs to vocab \n",
        "process_docs('txt_sentoken/pos', vocab, 0) \n",
        "process_docs('txt_sentoken/neg', vocab, 0) \n",
        "# print the size of the vocab \n",
        "print(len(vocab)) \n",
        "# keep tokens with a min occurrence min_occurane = 2 \n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane] \n",
        "print(len(tokens)) \n",
        "# save tokens to a vocabulary file \n",
        "save_list(tokens, 'vocab.txt')\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbefm9p27jSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat vocab.txt"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75fODRLnx6Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|def process_docs(directory,vocab, is_train):\n",
        "  lines= list()\n",
        "  for filename in listdir(directory):\n",
        "    if is_train and filename.startswith('CV9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    path= directory + '/'+filename\n",
        "    line= doc_to_lines(path, vocab)\n",
        "    lines.append(line)\n",
        "    return lines\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s_ahn9EzHcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_clean_dataset(is_train,vocab):\n",
        "  neg= process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "  pos= process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\n",
        "  docs=neg+pos\n",
        "\n",
        "  labels = [0 for _ in range(len(neg))] +[1 for _ in range(len(pos))]\n",
        "  return docs , labels\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozB68MyI0AMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(n_words):\n",
        "  model= Sequential()\n",
        "  model.add(Dense(50,activation='relu'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  model.comple(loss='binary_crossentropy',optimizer='ADAM',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnnyaiKV2XTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer(lines): \n",
        "  tokenizer = Tokenizer() \n",
        "  tokenizer.fit_on_texts(lines) \n",
        "  return tokenize"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-R2i9jv0qAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores= list()\n",
        "  n_repeat=10\n",
        "  n_words=Xtest.shape[1]\n",
        "  for i in range(n_repeat):\n",
        "    model=define_model(n_words)\n",
        "    model.fit(Xtrain,ytrain,epochs=10, verbose=0)\n",
        "    _, acc=model.evaluate(Xtest,ytest,verbose=0)\n",
        "    scores.append(acc)\n",
        "    print('%d accuracy: %s' % ((i+1), acc))\n",
        "    return scores\n",
        "\n",
        "def predict_sentiment(review,vocab,tokenizer,model):\n",
        "  tokens=clean_doc(review)\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  line = ' '.join(tokens)\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0: \n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnl3FpOo15IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}